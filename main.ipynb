{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterization of planet atmospheres with SPIRou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this tutorial is to provide the user with simple tools to robustly analyse sequences of spectra collected with SPIRou during a planetary transit. It is based on a publicly-available standard sequence of 96 observations of the variable M2 star <b>Gl 15A</b> (also named <b>V* GX And</b>, or <b>HD 1326</b>, see http://simbad.u-strasbg.fr/simbad/sim-id?Ident=Gl%2015%20A). Note that the original sequence of spectra contains 192 spectra and we decided to reduce it by half to save computational time for this tutorial (the full sequence can be access on the Canadian Astronomy Data Center: https://www.cadc-ccda.hia-iha.nrc-cnrc.gc.ca/en/; date: 2020-10-08, PI: Donati, DRS version: 0.6.132). \n",
    "\n",
    "\n",
    "### Goal of the turorial\n",
    " As no transiting planet is known to orbit Gl 15A, we will inject a realistic synthetic planet atmosphere signature into the observed sequence of spectra. Then, assuming that the planet ephemerides are known, we will aim at recovering the injected planet atmosphere spectrum using a correlation analysis. This tutorial is thus divided into 3 main sections:\n",
    "\n",
    "\n",
    "\n",
    "0. **Preparing the data**\n",
    "\n",
    "    0.1. Read a time series of telluric-corrected SPIRou files in a given directory (here: Data/). This code assume that the spectra are corrected for telluric contamination using the method of [Artigau et al. 2014](https://ui.adsabs.harvard.edu/abs/2014SPIE.9149E..05A/abstract) (**f.fits** files). For each spectrum, we access the wavelength solution, spectrum, blaze solution and estimated telluric spectrum for each of the **49** SPIRou orders. For more information about SPIRou, please have a look to [Donati et al. 2020](https://ui.adsabs.harvard.edu/abs/2020MNRAS.498.5684D/abstract) and to the fantastic [SPIRou webpage](http://spirou.irap.omp.eu/) (see also incoming article about the SPIRou data reduction software or DRS, from Cook et al., in prep.).\n",
    "    \n",
    "    0.2. Visualize the data sets as well as relevant time series for the analysis: S/N per order, airmass, barycentric Earth radial velocity (BERV), transit ephemerides.\n",
    "    \n",
    "    0.3. Inject synthetic planet signature. We computed self-consistent atmospheric models of a hot Jupiter (Teq=1500K, metallicity= 1xsolar) and a warm Neptune (Teq=600K, metallicity=30xsolar) using Exo-REM ([Blain et al. 2021](https://www.aanda.org/articles/aa/full_html/2021/02/aa39072-20/aa39072-20.html), available online at: https://gitlab.obspm.fr/dblain/exorem). The calculations include non-equilibrium chemistry. From the outputs of Exo-REM, we produced high-resolution transit spectra (R=750000) with a line-by-line model. We produced also molecular templates (H2O, CH4, CO and NH3) corresponding to their contribution in the transit spectra. \n",
    "    Then, we simply build a synthetic sequence of spectra by multiplying the planet atmosphere template (i.e. wavelength-dependent transit depth) by the transit window (i.e. portion of the stellar covered by the planet) at every epoch of observation. We then multiply the input (blaze subtracted) data by the synthetic sequence of spectra, shifted into the Geocentric frame beforehand.\n",
    "    \n",
    "    \n",
    "\n",
    "1. **Data reduction process: removing telluric and stellar spectra from the observed sequence and normalize the spectra**\n",
    "\n",
    "    1.1. Exclude regions of strong telluric contaminations to reduce potential systematics in the analysis. This follows the method of [Boucher et al. 2021](https://ui.adsabs.harvard.edu/abs/2021arXiv210808390B/abstract)\n",
    "    \n",
    "    1.2. Interpolate and shift the spectra into the stellar rest frame (correct for BERV and stellar systemic RV and planet-induced signature)\n",
    "    \n",
    "    1.3. Remove the median spectrum from each observed spectrum\n",
    "    \n",
    "    1.4. Jointly normalize the spectra and remove strong outliers\n",
    "    \n",
    "    1.5. Detrend with airmass to filter most of the residuals of airmass-dependent features\n",
    "    \n",
    "    1.6. Reject bad pixels\n",
    "    \n",
    "    1.7. Remove residuals of correlation noise using a Princical component analysis (PCA) based approach\n",
    "\n",
    "\n",
    "\n",
    "2. **Retrieve the planet signature**\n",
    "\n",
    "    We propose the retrieve the injected planet atmosphere spectrum using a matching template analysis (often falsely denoted as cross-correlation analysis). The reduced sequence of spectra is iteratively compared to synthetic sequences of spectra created by shifting a given planet atmosphere template to different planet RV solutions (based on a grid of planet RV semi-amplitude Kp and RV at mid-transit Vsys). For each value of Kp and Vsys, we compute the associated planet RV, and create a synthetic sequence of spectra by (i) shifting the planet model according to the RV solution and (ii) weighting it by the transit window at each observing time. We then compute the correlation between the synthetic map and the observed sequence of spectra. This yields a correlation map in the (Kp,Vsys) space that can be converted to a 'significance' map. If the template matches the observations for the expected values of Kp and Vsys, this will reinforce the idea that the planet signal is indeed present in the data. By repeating the process for different planet atmosphere templates, we can do model comparison by comparing the obtained significance maps. A more robust way to do so is to perform a Markov Chain Monte Carlo process using the correlation-to-likelihood framework of [Brogi & Line 2019](https://ui.adsabs.harvard.edu/abs/2019AJ....157..114B/abstract). A more complete introduction about the analysis of exoplanet atmospheres at high resolution can be found in the review of [Birkby 2018](https://ui.adsabs.harvard.edu/abs/2018arXiv180604617B/abstract). \n",
    "    \n",
    "    \n",
    "    \n",
    "### Structure of the code\n",
    "\n",
    "The code is organised as follows\n",
    "\n",
    "- All the data used in the code are stored in the directory **Data/T_files**. Note that the names of the files are assumed to be ordered in the chronological order. This condition is systematially fulfilled for DRS-provided SPIRou files but it might be worth checking that the observational time is an inceasing function.\n",
    "\n",
    "- The models used in the tutorial are stored in the directory **Models/**. In this version, the format of each planet template is a 2-column file containing (1) the wave number in cm^(-1) and (2) the transit depth. This can be easily modified in Section 1.3 of the notebook.\n",
    "\n",
    "- The data reduction process is applied independently to each of the 49 SPIRou orders. We chose to work with a list of objects named **Order**  containing all attributes and relevant method for a given SPIRou order. All information about the Order object can be found in the **src.py** python file. In addition, more generic functions (not linked to the Order object) can be found in the lower part of the file.\n",
    "\n",
    "- Once the stellar and telluric spectra have been removed from each order in Section 1, the correlation process (matching template) is applied to a user-provided input list of orders. The process takes typically about 1 hour to combine all orders (~200 000 data points). To gain time in this tutorial, we propose to combine no more than a few orders (3-7). Note that we however recommend to combine as many orders, if relevant (e.g., contains enough planet signal and not too polluted by tellurics). All functions associated to the matching template algorithm can be found in the **correlation.py** python file.\n",
    "\n",
    "- All functions related to the plots shown in this notebook can be accessed in the **plots.py** python file.\n",
    "\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "No major dependencies is required to make the code run, but make sure that you use:\n",
    "- python3 (>3.5 typically)\n",
    "- scipy + astropy modules\n",
    "- pandas python module\n",
    "- If you want to compute a transit flux (optional), you will need to install the **batman** python module: https://lweb.cfa.harvard.edu/~lkreidberg/batman/\n",
    "\n",
    "\n",
    "\n",
    "### Questions ?\n",
    "\n",
    "If you have any question about the code please contact:\n",
    "- Baptiste Klein, baptiste.klein@physics.ox.ac.uk\n",
    "- Florian Debras, florian.debras@irap.omp.eu\n",
    "- Benjamin Charnay, benjamin.charnay@obspm.fr\n",
    "    \n",
    "\n",
    "\n",
    "<font color='pink'>######### Have fun during this tutorial! ##################</font>    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dependencies of the notebook\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import copy\n",
    "from src import *\n",
    "from plots import *\n",
    "from correlation import *\n",
    "font = {'size'   : 16,\n",
    "        'weight': 'light'}\n",
    "axes = {'labelsize': 16,\n",
    "        'labelweight': 'light'}\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('axes', **axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Read data\n",
    "\n",
    "This section aims at reading the input SPIRou observations and injecting a synthetic planet atmosphere spectrum therein. \n",
    "<font color='red'>A few notes:</font>\n",
    "- The cell below contains the main parameters to read the data and compute a realistic planet template. \n",
    "- In this example, the planet properties are based on the hot Jupiter **HD 189733 b**. In order to inject a realistic planet signature in the data, we need to compute a so-called transit window given the fraction of the planet occulting the star. Usually, this is done by computing a transit curve (e.g., using the python [batman module](https://lweb.cfa.harvard.edu/~lkreidberg/batman/). Since the installation of the module can be quite tricky depending on the distribution of your laptop, we pre-computed a transit curve (see file **transit_flux.dat**). Note however that the function **compute_transit** (in **src.py**) can be uncommented and used for any input planet parameters. Since a transit curve is already computed, the following star/planets parameters are actually not used in the code: Rp, Rs, ap, ep, ip, wp, limb dark, uh. Note: a realistic estimate of the stellar limb-darkening coefficients can be found on [this website](https://vizier.u-strasbg.fr/viz-bin/VizieR-4).\n",
    "- <font color='red'>Important note:</font> In this tutorial, SPIRou orders are designated by the absolute index. In other words, the reddest order is Order 31 whereas the bluest is Order 79. **Note that the mean wavelength corresponding to each order number can be found in the output of Section 1.1**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# GLOBAL PARAMETERS\n",
    "dir_data   = \"Data/T_files\"   ### Directory containing all \n",
    "orders     =  np.arange(31,80)[::-1].tolist() ### List of all orders analysed\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# STELLAR / PLANET PARAMETERS\n",
    "Ks        = 0.205                            #Planet semi-amplitude [km/s]\n",
    "T0        = 2459130.8962180                  #Mid-transit time [BJD]\n",
    "Porb      = 2.21857545                       #Planet orbital period [d]\n",
    "V0        = 11.73                            #Stellar Systemic Vel [km/s]\n",
    "\n",
    "### Used to compute transit window only\n",
    "Rp        = 82914.446                        #Planet radius [km]   \n",
    "Rs        = 528034.0                         #Stellar radius [km]\n",
    "ap        = 8.839304998                      #Semi-major axis [stellar radius]\n",
    "ep        = 0.0041                           #Orbital eccentricity\n",
    "ip        = 85.710                           #Orbital inclination [deg]\n",
    "wp        = -24.1                            #Argument of the periapsis [deg] \n",
    "limb_dark = \"quadratic\"                      #limb darkening model (\"nonlinear\", \"quadratic\", \"linear\")\n",
    "uh        = [0.0788,0.2679]                  #Values of limb-darkening coefficients\n",
    "#### Accessible here --> https://vizier.u-strasbg.fr/viz-bin/VizieR-4 \n",
    "\n",
    "### Transit flux\n",
    "dir_transit = \"transit_flux.dat\"             #Pre-computed transit flux values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1. Read and pre-process data\n",
    "\n",
    "In this section, we perform the following pre-processing steps:\n",
    "- Read all data present in the directory **Data/T_files** (parameter **dir_data**), assuming that the files are ordered in the chronological order\n",
    "- These files are DRS **t.fits** files, computed using the method described in [Artigau et al. 2014](https://ui.adsabs.harvard.edu/abs/2014SPIE.9149E..05A/abstract) (optimized for SPIRou --> see Cook et al., in prep. about the SPIRou DRS). In these files, regions of strong telluric absorptions are replaced by NaNs (to avoid insecure estimates of the stellar flux). Before working with the data, we flag all pixels for which a NaN value appear once of more. These pixels are removed from the observations.\n",
    "- We compute the transit window from the transit flux for the simulation planet and spot the start-of-transit and end-of-transit dates\n",
    "- We compute the RV correction between the Geocentric (or Tropospheric) frame (where the SPIRou spectra are taken) and the stellar rest frame. The RV correction Vc is given by: Vc = V0 + Vp - berv, where V0 is the systemic velocity of the star, Vp is the planet signature induced on the host star and berv is the barycentric Earth RV. Note that, in this example, we do not include Vp as the planet is not acutally present in the data. Note also that the RV signature of the binary companion is neglicted in this analysis due to its very small long-term effect (see [Pinamonti et al. 2018](https://ui.adsabs.harvard.edu/abs/2018A%26A...617A.104P/abstract) for more infrmation about the system).\n",
    "- We finally compute the planet orbital phase given by: phase = (T_obs-T0)/Porb, where T_obs are the observing times, T0 is the mid-transit time, and Porb is the planet orbital period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6146a21adf2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlist_ord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOrder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m### Initialize list of Order objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlist_ord\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mairmass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT_obs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mberv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msnr_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data_spirou\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist_ord\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mnobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DONE\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tutorial_atmosphere-main/src.py\u001b[0m in \u001b[0;36mread_data_spirou\u001b[0;34m(repp, list_ord, nord)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mi\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdul_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# intensity spectrum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0mw\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdul_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# wavelength vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mbla\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdul_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# blaze vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0matm\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdul_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# telluric spectrum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### READ DATA - SUBTRACT BLAZE - REMOVE NaNs\n",
    "\n",
    "print(\"\\nRead data\")\n",
    "list_ord = []\n",
    "nord     = len(orders)\n",
    "for kk in range(nord):\n",
    "    list_ord.append(Order(orders[kk])) ### Initialize list of Order objects\n",
    "list_ord,airmass,T_obs,berv,snr_mat = read_data_spirou(dir_data,list_ord,nord)\n",
    "nobs = len(T_obs)\n",
    "print(\"DONE\\n\")\n",
    "\n",
    "### Remove all NaNs and form array like structures\n",
    "print(\"\\nRemove NaNs\")\n",
    "cmp = 0\n",
    "for mm in range(nord):\n",
    "    O   = list_ord[cmp]   \n",
    "    err = O.remove_nan()\n",
    "    if err > 0: ### If only NaNs\n",
    "        print(\"Order\",O.number,\"empty - removed\")\n",
    "        del orders[cmp]\n",
    "        del list_ord[cmp]\n",
    "    else: cmp += 1\n",
    "nord = len(list_ord)\n",
    "print(\"DONE\\n\")\n",
    "\n",
    "\n",
    "print(\"\\nCompute transit curve\")\n",
    "flux        = np.loadtxt(dir_transit,skiprows=1)[:,1]\n",
    "window      = (1-flux)/np.max(1-flux)\n",
    "n_ini,n_end = get_transit_dates(flux)\n",
    "### Manual computation of the transit curve -- requires batman module\n",
    "# flux = compute_transit(Rp,Rs,ip,T0,ap,Porb,ep,wp,limb_dark,uh,T_obs)\n",
    "print(\"DONE\\n\")\n",
    "\n",
    "\n",
    "print(\"\\nCompute velocity correction - Geocentric frame to stellar rest frame\")\n",
    "Vp     =   get_rvs(T_obs,Ks,Porb,T0)  #[km/s]\n",
    "Vc     =   V0 - berv \n",
    "#Vc     =  V0 + Vp - berv  ### When working with real data \n",
    "phase  = (T_obs - T0)/Porb\n",
    "phase -= int(phase[-1])   \n",
    "print(\"DONE\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.2. Plot data\n",
    "In this section, we simply visualize the data. We plot the following quantities:\n",
    "- **Various time series:** (1) the transit curve which shows the fraction of the planetary transit covered by the observations, (2) the airmass, (3) the RV correction to move from the Geocentric frame to the stellar rest frame and (4) the order with maximum S/N (here, very high S/N of 260-300)\n",
    "- **Distribution of S/N in time-order space**\n",
    "- **A visualisation of the sequence of spectra collected for a given order** in the time-wavelength space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make plots\n",
    "\n",
    "# Plot time series\n",
    "plot_timeseries(T_obs,T0,Porb,phase,flux,n_ini,n_end,airmass,Vc,snr_mat)\n",
    "\n",
    "# Plot S/N distribution\n",
    "epoch = np.arange(nobs)\n",
    "lab   = [\"Order\",\"Epoch\",\"S/N\"]\n",
    "cmap  = \"PuOr_r\"\n",
    "plot_2D(orders,epoch,snr_mat,lab,cmap,n_ini,n_end)\n",
    "\n",
    "# Plot data and atmosphere spectrum for a given order\n",
    "numb  = 47    # 31 --> reddest; 79 --> bluest\n",
    "lab   = [\"Wavelength\",\"Orbital phase\",\"Flux\"]\n",
    "cmap  = \"gist_heat\"\n",
    "oo    = int(np.argmin(np.abs(np.array(orders,dtype=float)-numb)))\n",
    "title = \"Raw data - order \" + str(list_ord[oo].number)\n",
    "size  = (10,7)\n",
    "plot_2D(list_ord[oo].W_raw,phase,list_ord[oo].I_raw,lab,cmap,n_ini,n_end,title,size)\n",
    "\n",
    "title = \"Telluric spectrum - order \" + str(list_ord[oo].number)\n",
    "# plot_2D(list_ord[oo].W_raw,phase,list_ord[oo].I_atm,lab,cmap,n_ini,n_end,title,size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Add synthetic planet signature \n",
    "\n",
    "We now add a synthetic planet signature to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate below the name of the planet atmosphere template injected in the data\n",
    "name_model        = \"Model/Jupiter_1sol_Teq1500K_Tint150K\"\n",
    "#name_model        = \"Model/Neptune_30sol_Teq600K_Tint50K\"\n",
    "aa                = np.loadtxt(name_model)\n",
    "\n",
    "W_mod_inj         = np.array(1./aa[:,0]*1e7)[::-1]\n",
    "transit_depth_inj = np.array(aa[:,1])[::-1]\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(W_mod_inj,transit_depth_inj,\"-k\",lw=.5)\n",
    "plt.ylabel(\"Transit Depth\")\n",
    "plt.xlabel(\"Wavelength [nm]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the semi-amplitude K_inj of the RV of the injected planet signature and the RV at mid-transit, V_inj. K_inj can be obtained from the star planet mass ratio (q = Ms/Mp) and the semi-amplitude of the RV signature induced by the planet of the host star, Ks, such that: **K_inj = q * Ks**. V_inj is expected to be close to 0 (non-zero values are interpreted as the presence of stellar winds towards or away from the observer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Inject the planet signature into the data\n",
    "\n",
    "K_inj       = 154.0   ### Semi-amplitude of the injected planet RV (Jupiter)\n",
    "#K_inj       = 61.6   ### Semi-amplitude of the injected planet RV (Neptune)\n",
    "\n",
    "V_inj       = 0.0     ### RV at mid-transit of the injected planet\n",
    "numb        = 47      ### Select here the order for which you want to plot the synthetic planet signature\n",
    "\n",
    "oo          = int(np.argmin(np.abs(np.array(orders,dtype=float)-numb)))\n",
    "discard     = np.zeros(nord)\n",
    "list_ord_pl = copy.deepcopy(list_ord) # Independent copy of the list of Order objects\n",
    "\n",
    "for nn in range(nord):\n",
    "    # Select and stored the synthetic data in a wavelength range encompassing each order\n",
    "    O         = list_ord_pl[nn]\n",
    "    Wmin,Wmax = 0.95*O.W_raw.min(),1.05*O.W_raw.max() # Take a wavelength range +/-5% larger than data\n",
    "    indm      = np.where((W_mod_inj>Wmin)&(W_mod_inj<Wmax))[0]\n",
    "    W_sel     = W_mod_inj[indm]\n",
    "    I_sel     = transit_depth_inj[indm]\n",
    "    \n",
    "    # If some parts of the data are not covered by the model, discard the order\n",
    "    if np.min(W_sel) > 0.99*np.min(O.W_raw) or np.max(W_sel) < 1.01*np.max(O.W_raw):\n",
    "        print(\"Order\",O.number,\"incomplete -- discarded\")\n",
    "        discard[nn] = 1\n",
    "    else:\n",
    "        O.add_planet(W_sel,I_sel,window,phase,K_inj,V_inj,Vc)\n",
    "        O.I_raw = O.I_raw_pl\n",
    "    \n",
    "    ### Display requested order\n",
    "    if nn == oo:\n",
    "        lab   = [\"Wavelength\",\"Orbital phase\",\"Transit depth\"]\n",
    "        cmap  = \"gist_heat\"\n",
    "        size  = (10,7)\n",
    "        title = \"Planet atmosphere signal\"\n",
    "\n",
    "        plot_2D(O.W_raw,phase,O.I_syn,lab,cmap,n_ini,n_end,title,size)\n",
    "\n",
    "        ### Uncomment the line below to visualize the shift of the absorption lines during the transit\n",
    "#         I_test = O.I_syn/(np.dot(np.mean(O.I_syn,axis=1).reshape(len(O.I_syn),1),np.ones((1,len(O.I_syn[0])))))\n",
    "#         plot_2D(O.W_raw[1000:1500],phase,I_test[:,1000:1500],lab,cmap,n_ini,n_end,title,size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Exclude strong telluric contamination and discard orders not covered by template\n",
    "\n",
    "We start by removing regions located within strong telluric absorption lines following the method introduced in [Boucher et al. 2021](https://ui.adsabs.harvard.edu/abs/2021arXiv210808390B/abstract). From the median DRS-provided Earth atmosphere spectrum, we flag all telluric lines with a relative absorption larger than dep_min (conservative value of 40% in [Boucher et al. 2021](https://ui.adsabs.harvard.edu/abs/2021arXiv210808390B/abstract) and set to 50%). We then remove points on both sides of each flagged absorption line until reaching a relative level of 1-thres_up with respect to the continuum. Orders with less than Npt_lim points remaining after the telluric exclusion are discarded from the analysis (typically ~6 orders with water absorption bands -- orders 55-57 and 40-42). \n",
    "\n",
    "\n",
    "**NOTE:** We provide here the list of **all SPIRou orders** and their associated mean wavelength. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dep_min  = 0.5\n",
    "thres_up = 0.05\n",
    "### Remove regions of strong telluric absorption using Boucher et al. 2021 method\n",
    "for nn in range(nord):\n",
    "    list_ord_pl[nn].remove_tellurics(dep_min,thres_up)\n",
    "    \n",
    "### Remove all orders with less then Npt_lim pts remaining\n",
    "### and display infos\n",
    "Npt_lim = 500\n",
    "npt_rem = np.arange(nord)\n",
    "npt_ini = np.arange(nord)\n",
    "ind_rem = []\n",
    "ord_sta = [\"ok\" for i in range(nord)]\n",
    "wmean   = np.arange(nord)\n",
    "\n",
    "for nn in range(nord):\n",
    "    O           = list_ord_pl[nn]\n",
    "    \n",
    "    npt_ini[nn] = len(O.W_raw)   \n",
    "    npt_rem[nn] = len(O.W_raw) - len(O.W_cl)\n",
    "    wmean[nn]   = O.W_mean    \n",
    "    if discard[nn]>0:\n",
    "        ind_rem.append(nn)\n",
    "        ord_sta[nn] = \"NOT COVERED\"\n",
    "    elif len(O.W_cl) <= Npt_lim:\n",
    "        ind_rem.append(nn)\n",
    "        ord_sta[nn] = \"DISCARDED\"        \n",
    "list_ord_pl_fin =  np.delete(list_ord_pl,ind_rem)\n",
    "nord_fin        = len(list_ord_pl_fin)\n",
    "orders_fin      = np.delete(orders,ind_rem)\n",
    "        \n",
    "### Display results\n",
    "infos = np.array([orders,wmean,npt_ini,npt_rem,ord_sta]).T\n",
    "col   = [\"Order\",\"W_mean [nm]\",\"Nb pts\",\"Nb pts removed\",\"Status\"]\n",
    "dtf   = pd.DataFrame(infos, columns=col)\n",
    "print(\"List of the orders selected for the analysis:\")\n",
    "print(dtf)\n",
    "\n",
    "### Display results\n",
    "plot_orders(npt_rem,wmean,orders,ind_rem,laby=\"Points removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Align spectra in the stellar rest frame\n",
    "\n",
    "The data reduction procedure works with stellar spectra in the stellar rest frame. As the data are corrected for most of the telluric contamination (**t.fits** files), the spectra are dominated by the stellar contribution. As a consequence, working in the stellar rest frame allows us to remove the stellar contribution in a more efficient way than in the Geocentric frame (as the position of each stellar spectrum depends on the BERV). Note also that, when working with SPIRou e2ds files (or e.fits extensions), we may consider to do the data reduction process in the Geocentric frame. In this case, this will allow us to (i) remove more efficiently the telluric lines (aligned in the Geocentric frame) and (ii) avoid interpolating the observed spectra (no interpolation error added to the data).\n",
    "\n",
    "\n",
    "We move the observed sequence of spectra from the Geocentric frame to the stellar rest frame, we process as follows:\n",
    "- We interpolate each spectrum using scipy interp1d function (choose the type of interpolation by changing the parameter called \"type\"). \n",
    "- We shift the spectrum according to the velocimetric solution Vc computed in Section 0.1. Each value of the shifted spectrum is taken by intergrating the interpolation over one SPIRou pixel. \n",
    "- To avoid any extrapolation error, we remove the first and last **n_bor** points of each spectrum (note that, due to the blaze grating's structure, these points lie in regions of much higher noise level anyway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame    = \"stellar\"  ### \"stellar\" / \"barycentric\" vs \"earth\" / \"geocentric\"\n",
    "n_bor    = 30\n",
    "kind     = \"linear\"    ### \"cubic\" or \"linear\" --> see scipy interp1d page\n",
    "print(\"\\nInterpolate and align all spectra\")\n",
    "if frame == \"stellar\" or \"barycentric\":\n",
    "    for nn in range(nord_fin):\n",
    "        list_ord_pl_fin[nn].align(Vc,kind,n_bor)\n",
    "print(\"DONE\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Remove reference spectrum\n",
    "\n",
    "The first step of the data reduction process consists in computing a high-S/N planet-free spectrum and remove it from each observed spectrum. In the stellar rest frame, the position of the stellar lines is constant (to the first order) during the observed sequence. In contrast, the position of the planet atmosphere lines is shifted by several km/s (several pixels) during the transit. As a consequence, the median observed spectrum should enclose most of the star contribution along with a negligible planet atmosphere contribution. To ensure that no planet atmosphere spectrum is enclosed in the median spectrum (especially when the RV semi amplitude of the planet is relatively low, e.g., for close-in planets less massive than Neptune), one option is to compute the median spectrum using only the out-of-transit spectra, but one has to ensure to have enough spectra in the baseline, in order to maximise the S/N of the reference spectrum.\n",
    "\n",
    "In practice, we give the user the ability to select if the median spectrum is computed on all observations (**mode**=\"full\") or on the out-of-transit spectra only (**mode**=\"out\"). Once the median spectrum I_med is computed, we adjust I_med to each observed spectrum using a simple Least-squared estimator and divide the observed spectrum by the best-fitting prediction. We propose below to visualise the best-fitting solution for a given order (parameter **numb**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode        = \"out\"  ### Compute median spec on \"full\" or \"out\"-of transit spectra\n",
    "numb        = 47      ### Order for which we plot the prediction\n",
    "oo          = int(np.argmin(np.abs(np.array(orders_fin,dtype=float)-numb)))\n",
    "\n",
    "for kk in range(nord_fin):\n",
    "    if kk == oo:\n",
    "        list_ord_pl_fin[kk].subtract_stellar(mode,n_ini,n_end,True)        \n",
    "    else:\n",
    "        list_ord_pl_fin[kk].subtract_stellar(mode,n_ini,n_end,False)\n",
    "        \n",
    "\n",
    "#         lab   = [\"Wavelength\",\"Orbital phase\",\"Flux\"]\n",
    "#         cmap  = \"gist_heat\"\n",
    "#         title = \"Raw data - order \" + str(list_ord_fin[kk].number)\n",
    "#         size  = (10,7)\n",
    "#         plot_2D(list_ord[oo].W_al,phase,list_ord[oo].I_sub,lab,cmap,n_ini,n_end,title,size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Normalize the residual spectra\n",
    "\n",
    "The resulting time series of spectra still exhibit low-frequencies structures due to correlated noise during the sequence (e.g., modal noise). Correcting for these trend as well as homogeneising the level of flux in each spectrum is a mandatory step in the correlation analysis. \n",
    "\n",
    "For each spectrum of each order, we perform the following iterative process:\n",
    "- We apply a median filter (moving-median)\n",
    "- Divide the spectrum by the moving median\n",
    "- Conduct a sigma clipping to identify outliers\n",
    "- For each flagged outlier, we remove a few adjacent points to correct for any potential pixel contamination\n",
    "- We repeat the process until no outlier is identified\n",
    "\n",
    "**WARNING:** This process typically takes a few min to run on all orders\n",
    "\n",
    "**NOTE:** *There is no robust rule to tune the median filter here. Tests with synthetic data have shown that the results were marginally-impacted by variations of ~100 in N_med, as long as N_med is not too low or too large. One method to tune N_med for a given spectrum, would be to monitor the evolution of the dispersion of this spectrum normalized by the median filter for a range of N_med values. As N_med decreases (starting for abritrarily large values), the dispersion of the normalized spectrum will first decrease (correction of the low-frequency trends), reach a plateau, and decrease again (fit for white noise).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_med   = 200   # Number of points for the median filter\n",
    "sig_out = 5     # Threshold for the sigma clipping [in sigma]\n",
    "N_adj   = 2     # Number of adjacent points removed for each outlier identified \n",
    "numb    = 47    # Index of the order displayed\n",
    "N_bor   = 30    # Number of points removed at each side of eahc order\n",
    "oo      = int(np.argmin(np.abs(np.array(orders_fin,dtype=float)-numb)))\n",
    "time0   = time.time()\n",
    "\n",
    "print(\"\\nNormalize residual spectra and remove outliers\")\n",
    "for kk in range(nord_fin):\n",
    "    if kk != oo: plot = False\n",
    "    else: plot = True\n",
    "    if kk%10==0.0: print(\"Order n.\",kk+1,\"/\",nord_fin)\n",
    "    list_ord_pl_fin[kk].normalize(N_med,sig_out,N_adj,N_bor,plot)\n",
    "print(\"DONE\\n\")\n",
    "time1 = time.time()\n",
    "print(\"Duration:\",(time1-time0)/60.,\"min\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5. Detrending with airmass\n",
    "\n",
    "Time-dependent residuals of absorption lines are still observed on the normalized spectra (residuals of strong lines whose absorption vary with airmass). A physically-motivated approach to correct for this time-varying structures is to **detrend the sequence of normalized spectra with airmass** (see [Brogi et al. 2016](https://ui.adsabs.harvard.edu/abs/2016ApJ...817..106B/abstract) or [Brogi et al. 2018](https://ui.adsabs.harvard.edu/abs/2018A%26A...615A..16B/abstract)). Although several more or less complex methods to detrend with airmass exist in the literature (e.g., in log space), we present here a very simplistic method consisting in modelling each normalized spectrum, $I_{\\rm{n}}(t)$, by\n",
    "\n",
    "$I_{\\rm{n}}(t)$ = $\\boldsymbol{I_{0}}$ + $\\boldsymbol{I_{1}}$ $A$ + $\\boldsymbol{I_{2}}$ $A^{2}$\n",
    "\n",
    "where $A(t)$ is the airmass value at time $t$. This linear problem can be analytically solved using a least-squares estimator. Note that, in practice, such a simple model is not expected to fully describe time-dependent residuals within the sequence of spectra. In particular, the water absorption from the Earth atmosphere depends on more complex parameters than the airmass (see [Brogi et al. 2018](https://ui.adsabs.harvard.edu/abs/2018A%26A...615A..16B/abstract)). One alternative method is to directly apply statistical methods like principal component analysis in order to remove most of correlated noise from the normalized spectra (see [de Kok et al. 2013](https://ui.adsabs.harvard.edu/abs/2013A%26A...554A..82D/abstract) and Section 1.7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg     = 2   ### Degree of the airmass model\n",
    "numb    = 47  ### Plot best prediction for a given order\n",
    "oo      = int(np.argmin(np.abs(np.array(orders_fin,dtype=float)-numb)))\n",
    "\n",
    "print(\"\\nDetrending with airmass\")\n",
    "for kk in range(nord_fin):\n",
    "    pb, I_pred = list_ord_pl_fin[kk].detrend_airmass(airmass,deg)\n",
    "    \n",
    "    if kk == oo:\n",
    "        O = list_ord_pl_fin[kk]\n",
    "        plt.figure(figsize=(9,7))\n",
    "        X,Y = np.meshgrid(O.W_norm,phase)\n",
    "        plt.pcolor(X,Y,I_pred,cmap=\"gist_heat\")\n",
    "        plt.xlabel(\"Wavelength [nm]\")\n",
    "        plt.ylabel(\"Orbital phase\")\n",
    "        title = \"Best-fitting airmass detrending solution - Order \" + str(O.number)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(15,5))\n",
    "        for nn,pp in enumerate(pb):\n",
    "            lab = r\"I$_{\" + str(nn) + \"}$\" \n",
    "            plt.plot(O.W_norm,pp,label=lab)\n",
    "        plt.legend(ncol=3)\n",
    "        plt.show()\n",
    "    \n",
    "#         lab   = [\"Wavelength\",\"Orbital phase\",\"Flux\"]\n",
    "#         cmap  = \"gist_heat\"\n",
    "#         oo    = int(np.argmin(np.abs(np.array(orders,dtype=float)-numb)))\n",
    "#         title = \"Before airmass detrending - order \" + str(list_ord[oo].number)\n",
    "#         size  = (10,7)\n",
    "#         plot_2D(O.W_norm,phase,O.I_norm,lab,cmap,n_ini,n_end,title,size) \n",
    "        \n",
    "#         lab   = [\"Wavelength\",\"Orbital phase\",\"Flux\"]\n",
    "#         cmap  = \"gist_heat\"\n",
    "#         oo    = int(np.argmin(np.abs(np.array(orders,dtype=float)-numb)))\n",
    "#         title = \"After airmass detrending - order \" + str(list_ord[oo].number)\n",
    "#         size  = (10,7)\n",
    "#         plot_2D(O.W_norm,phase,O.I_det,lab,cmap,n_ini,n_end,title,size)       \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "print(\"DONE\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########################################################################################\n",
    "\n",
    "**BONUS 1:** [Brogi et al. 2018](https://ui.adsabs.harvard.edu/abs/2018A%26A...615A..16B/abstract) recommend to perform the airmass detrending in the log space (i.e., by working with the logarithm of the flux). Implement a new function *detrend_airmass(I_norm,airmass,deg)* performing the detrending with airmass on the logarithm of the attribute *I_norm* of each Order object of the  list *list_ord_pl_fin*. This function will return a detrended sequence of spectra I_det (in the non-log space) which will be stored in the attribute *I_det* of each Order object of the list *list_ord_pl_fin*. \n",
    "\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6. Filter bad pixels\n",
    "\n",
    "In this step, we filter bad pixels (or velocity bins) in each order. We first compute the dispersion of each pixel (along the time axis). In principle, the sequence of spectra should be dominated by white noise by now and, therefore, each observation can be roughly seen as different realization of the photon/instrument noise (we encourage the user to double check this information by e.g., applying a Fourrier transform to the data). As a consequence, we expect the distribution of dispersions along the time axis in each order to shaped like a parabola with: (i) a minimum around the order center (level of dispersion corresponding to the DRS-provided S/N) and (ii) increasing noise at the order extremities (due to the blaze function). We therefore fit a parabola to the distribution of dispersions along the time axis using an iterative outlier removal procedure. All outliers removed in the process are flagged as bad pixels and discarded from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numb    = 47  ### Index of order displayed \n",
    "oo      = int(np.argmin(np.abs(np.array(orders_fin,dtype=float)-numb)))\n",
    "deg_px  = 2   ### Degree of the polynomial fit\n",
    "sig_px  = 4.0 ### Threshold for outlier removal [in sigma]\n",
    "N_px    = 400 ### Size of window (in nb of points) centered on the order center, on which the empirical \n",
    "              ### spectrum S/N is computed\n",
    "print(\"\\nFilter bad pixels\")\n",
    "for kk in range(nord_fin):\n",
    "    if kk == oo: list_ord_pl_fin[kk].filter_pixel(deg_px,sig_px,True)\n",
    "    else: list_ord_pl_fin[kk].filter_pixel(deg_px,sig_px,False)\n",
    "    list_ord_pl_fin[kk].get_pixel_dispersion(N_px)\n",
    "print(\"DONE\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare the dispersion empirically-computed from the center of each order to the DRS-provided photon noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Display global spectrum dispersion and compare it to DRS\n",
    "plot_spectrum_dispersion(list_ord_pl_fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7. Filter correlated noise with Singular Value Decomposition\n",
    "\n",
    "Although dominated by the white noise, the remaining sequence of spectra is still polluted by correlated noise (e.g., residuals of telluric/stellar spectra). If uncorrected, these components will harm the correlation process, with the result of masking or even mimicking a signature from the planet atmosphere. Hence the interest in using statistical methods, here, principal component analysis (PCA - in fact Singular Value Decomposition here) to remove a substantial fraction of the remaining correlated noise.\n",
    "\n",
    "More information about PCA can be found one the (pretty well explained) [wikipedia page](https://en.wikipedia.org/wiki/Principal_component_analysis) and its application to a sequence of nIR spectra is described [here](https://ui.adsabs.harvard.edu/abs/2018arXiv180604617B/abstract) and [here](https://ui.adsabs.harvard.edu/abs/2019ApJ...878..153D/abstract). Let's consider our reduced sequence of spectra, $I_{\\rm{red}}$, containing N$_{\\rm{obs}}$ spectra and N$_{\\rm{w}}$ wavelength bins. In the PCA framework, the sequence of spectra (in each order) is described as N$_{\\rm{obs}}$ realizations of N$_{\\rm{w}}$ random variables (centered and reduced beforehand). A PCA decomposition consists in projected the 2D matrix $I_{\\rm{red}}$ onto an orthonormal basis $\\mathcal{B}_{\\rm{pca}}$ whose vectors are the eigenvectors of the covarance matrix associated to $I_{\\rm{red}}$ and given by  $I_{\\rm{red}}^{T} I_{\\rm{red}}$. One can demonstrate that the larger the eigenvalue, the more the associated eigenvector (called PCA component) contributes to the variance budget of $I_{\\rm{red}}$. As a consequence, if $I_{\\rm{red}}$ is only drawn by uncorrelated noise, all eigen values will have the same order of magnitude. In contrast, correlated components will induce a discrepency in the distribution of eigenvalues. \n",
    "\n",
    "In practice, we use PCA to decompose $I_{\\rm{red}}$ into its N$_{\\rm{obs}}$ PCA components for each order and sort the eigenvalues (and associated eigenvectors) in decreasing order. We then chose the number of components associated to correlated noise (see next paragraph) and set them to 0, before projecting the PCA decomposition back into the original space. \n",
    "\n",
    "**Tuning the number of components to remove:** This step is critical in order to remove most of the residuals of telluric and stellar spectra while preserving the planet atmosphere contribution. Note that the amplitude of the planet atmosphere absorption lines is generally 10-100 times lower than the typical white noise level. From injection tests, we can easily demonstrate that the presence of the planet atmosphere spectrum has only a marginal effect on the distribution of eigenvalues in the PCA decomposition. In this tutorial, we compute the contribution $C$ of each eigenvector to the total variance budget (i.e., $C(e_{i}) = e_{i}/\\Sigma_{j}e_{j}$, for all eigenvalue $e_{i}$). All eigenvalue $e_{i}$ for which $c(e_{i})$ is larger than a given user-provided threshold (set to 4% here) is said to be associated to correlated noise and the corresponding component is set to 0. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, you can visualize the first components associated to your favorite SPIRou order (select the order index with the parameter **numb** and the number of components displayed with **N_comp**). See the list of order indices and corresponding wavelengths in Section 1.1. You will noticed that the larger the component, the more complex the structures become (similar to taking the derivatives). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Plot the first components\n",
    "### Note: This cell is just here to visualize PCA components\n",
    "\n",
    "numb    = 47  ### Select index of order you want to display\n",
    "oo      = int(np.argmin(np.abs(np.array(orders_fin,dtype=float)-numb)))\n",
    "O       = list_ord_pl_fin[oo]\n",
    "N_comp  = 3   ### Show the first N_comp components \n",
    "var,I_pca,I_del = make_pca(O.I_red,N_comp,True)\n",
    "plt.figure(figsize=(20,6))\n",
    "X,Y = np.meshgrid(O.W_red,phase)\n",
    "for nn in range(N_comp):\n",
    "    ax = plt.subplot(1,N_comp,nn+1)\n",
    "    vi = np.mean(I_del[0]) - 3.*np.std(I_del[0])\n",
    "    vf = np.mean(I_del[0]) + 3.*np.std(I_del[0])\n",
    "    cc = plt.pcolor(X,Y,I_del[nn],cmap=\"gist_heat\",vmin=vi,vmax=vf)\n",
    "    if nn > 0: plt.yticks([])\n",
    "    if nn == 0: plt.ylabel(\"Orbital phase\")\n",
    "    if nn == N_comp-1:\n",
    "        cb   = plt.colorbar(cc,ax=ax)\n",
    "    title = \"Component \" + str(nn+1)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Wavelength [nm]\")\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main PCA process:** We know use the method described above to tune the number of components removed from the observed sequence of spectra for each order. Warning: this might take a few minutes to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Automatic scaling of PCA\n",
    "numb       = 47   ### Show the distribution of eigenvalues for this order\n",
    "oo         = int(np.argmin(np.abs(np.array(orders_fin,dtype=float)-numb)))\n",
    "threshold  = 0.04 ### threshold of variance contribution (above --> PCA components set to 0)\n",
    "N_comp_fin = np.zeros(nord_fin,dtype=int)\n",
    "wmean      = np.zeros(nord_fin)\n",
    "\n",
    "for nn in range(nord_fin):\n",
    "    \n",
    "    O_pl             = list_ord_pl_fin[nn]\n",
    "    evar,I_pca       = make_pca(O_pl.I_red,0)\n",
    "    indc             = np.where(evar>=threshold)[0]\n",
    "    wmean[nn]        = O_pl.W_mean\n",
    "    N_comp_fin[nn]   = len(indc)\n",
    "\n",
    "    if nn == oo:\n",
    "        XX = np.arange(len(evar)) + 1\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(XX,evar,\"+k\")\n",
    "        plt.axhline(0.04,ls=\":\",color=\"r\")\n",
    "        plt.xlabel(\"Component\")\n",
    "        plt.ylabel(\"Variance contribution\")\n",
    "        plt.show()\n",
    "        \n",
    "    evar,I_pca   = make_pca(O_pl.I_red,N_comp_fin[nn])\n",
    "    O_pl.I_pca   = I_pca\n",
    "    \n",
    "### Print number of components rejected for each order\n",
    "infos = np.array([orders_fin,wmean,N_comp_fin]).T\n",
    "col   = [\"Order\",\"W_mean [nm]\",\"Components removed\"]\n",
    "dtf   = pd.DataFrame(infos,columns=col)\n",
    "print(dtf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########################################################################################\n",
    "\n",
    "**BONUS 2:** Do you have any improvement the tuning the number of PCA components removed from each order? \n",
    "\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.8. Summary -- Visualize all the data reduction process for a given order\n",
    "\n",
    "Display the main steps of the data reduction process for a given order (change **numb** to change the order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numb = 47  ### Show the evolution of the sequence of spectra for this order\n",
    "O    = list_ord_pl_fin[int(np.argmin(np.abs(np.array(orders_fin,dtype=float)-numb)))]\n",
    "cmap = \"gist_earth\" # Another fancy color map?\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "###############################        \n",
    "ax   = plt.subplot(411)\n",
    "X,Y  = np.meshgrid(O.W_al,phase)\n",
    "Z    = O.I_sub\n",
    "zmin = O.I_red.mean() - 3.*O.I_red.std()\n",
    "zmax = O.I_red.mean() + 3.*O.I_red.std()\n",
    "c    = plt.pcolor(X,Y,Z,cmap=cmap,vmin=zmin,vmax=zmax) \n",
    "plt.colorbar(c,ax=ax)\n",
    "ax.set_ylabel(\"Orbital phase\")  \n",
    "ax.set_xticks([])        \n",
    "ax.set_xlim(np.min(O.W_red),np.max(O.W_red))\n",
    "\n",
    "tx = \"Removing reference spectrum: rms = \" + str(round(np.std(Z),4))                           \n",
    "plt.text(np.min(O.W_red)+1.5,0.8*np.max(phase),tx,color=\"w\",fontsize=20,fontweight=\"bold\")\n",
    "\n",
    "###############################  \n",
    "ax   = plt.subplot(412)\n",
    "X,Y  = np.meshgrid(O.W_norm,phase)\n",
    "Z    = O.I_norm\n",
    "zmin = O.I_red.mean() - 3.*O.I_red.std()\n",
    "zmax = O.I_red.mean() + 3.*O.I_red.std()\n",
    "c    = plt.pcolor(X,Y,Z,cmap=cmap,vmin=zmin,vmax=zmax)   \n",
    "ax.set_ylabel(\"Orbital phase\")  \n",
    "ax.set_xticks([])        \n",
    "ax.set_xlim(np.min(O.W_red),np.max(O.W_red))\n",
    "plt.colorbar(c,ax=ax)\n",
    "\n",
    "tx = \"Normalized - rms = \" + str(round(np.std(Z),4))                            \n",
    "plt.text(np.min(O.W_red)+1.5,0.8*np.max(phase),tx,color=\"w\",fontsize=20,fontweight=\"bold\")\n",
    "\n",
    "\n",
    "###############################  \n",
    "ax   = plt.subplot(413)\n",
    "X,Y  = np.meshgrid(O.W_red,phase)\n",
    "Z    = O.I_red\n",
    "zmin = O.I_red.mean() - 3.*O.I_red.std()\n",
    "zmax = O.I_red.mean() + 3.*O.I_red.std()\n",
    "c    = plt.pcolor(X,Y,Z,cmap=cmap,vmin=zmin,vmax=zmax)  \n",
    "ax.set_ylabel(\"Orbital phase\")  \n",
    "ax.set_xticks([])        \n",
    "ax.set_xlim(np.min(O.W_red),np.max(O.W_red))\n",
    "plt.colorbar(c,ax=ax)\n",
    "tx = \"Before PCA - rms = \" + str(round(np.std(Z),4))                           \n",
    "plt.text(np.min(O.W_red)+1.5,0.8*np.max(phase),tx,color=\"w\",fontsize=20,fontweight=\"bold\")\n",
    "\n",
    "\n",
    "###############################  \n",
    "ax   = plt.subplot(414)\n",
    "X,Y  = np.meshgrid(O.W_red,phase)\n",
    "Z    = O.I_pca\n",
    "zmin = O.I_red.mean() - 3.*O.I_red.std()\n",
    "zmax = O.I_red.mean() + 3.*O.I_red.std()\n",
    "c    = plt.pcolor(X,Y,Z,cmap=cmap,vmin=zmin,vmax=zmax)  \n",
    "ax.set_ylabel(\"Orbital phase\")  \n",
    "ax.set_xlim(np.min(O.W_red),np.max(O.W_red))\n",
    "plt.colorbar(c,ax=ax)\n",
    "ax.set_xlabel(\"Wavelength [nm]\")\n",
    "\n",
    "tx = \"Last - rms = \" + str(round(np.std(Z),4)) \n",
    "plt.text(np.min(O.W_red)+1.5,0.8*np.max(phase),tx,color=\"w\",fontsize=20,fontweight=\"bold\")\n",
    "plt.subplots_adjust(hspace=0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Planet recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we aim at recovered the planet atmosphere signal injected into the data using a template-matching process (often referred as cross-correlation process in the literature; see [Birkby 2018](https://ui.adsabs.harvard.edu/abs/2018arXiv180604617B/abstract)). The method is relatively simple:\n",
    "- Your inputs are the observed sequence of spectra (after data and PCA reduction) and a given 1D template of the planet atmosphere spectrum. The user provides a grid of Kp (semi-amplitude of planet RV) and Vsys (RV at mid-transit). \n",
    "- For each couple (Kp,Vsys), we compute the expected planet RV, **vp**, given by  (assuming a circular orbit): **vp** = Kp * sin(2$\\pi$ $\\Phi$) + Vsys; where $\\Phi$ is the planet orbital phase centered on the mid-transit time. We then build a sequence of spectra (sampled as the observations) by shifting the planet atmosphere template at each date *t* according to **vp**(*t*) and weighting it by the transit window. We then compute the correlation between the synthetic map and the observed sequence of spectra.\n",
    "- We repeat the process on all (Kp,Vsys) and obtain a map of correlation (Kp,Vsys). In order to estimate the significance of any peak in the correlation map, we need to estimate the level of correlation associated to pure white noise. This is done, in this tutorial, by computing the typical dispersion in regions of the correlation map where no planetary signal is found (typically, by excluding the regions close to K_inj and V_inv. More robust estimates of the significance can be obtained by \n",
    "    1. Computing the correlation map of the template with maps of white noise (possible amplified at the extremities of each order to account for the blaze function). The typical level of error associated to the correlation with the template is given by the dispersion in the correlation with white noise. \n",
    "    2. Using a Student-t test which provides a more robust framework to estimate the significance of the signal (see [Brogi et al. 2016](https://ui.adsabs.harvard.edu/abs/2016ApJ...817..106B/abstract) or [Birkby 2018](https://ui.adsabs.harvard.edu/abs/2018arXiv180604617B/abstract))\n",
    "    \n",
    "**NOTE:** In practice, we want to compare models of planet atmosphere assuming different conditions (e.g., P,T profile, wind dynamics) and compositions. Comparing significance maps does not provide a robust framework to perform a model comparison (and derive accurate error bars for the different parameters of the fit). In practice, the **correlation-to-likelihood** framework introduced in [Zucker 2003](https://ui.adsabs.harvard.edu/abs/2003MNRAS.342.1291Z/abstract) and recently adapted to high-resolution spectral analysis in [Brogi & Line 2019](https://ui.adsabs.harvard.edu/abs/2019AJ....157..114B/abstract) allows to perform the model comparison in the bayesian framework. This allows one to use a Bayesian Markov Chain Monte Carlo (MCMC) to obtain a posterior distribution for the model parameters from which error bars can be estimated. Note that this process takes time (thus beyond the second of this tutorial) and should be parallelise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  What model to you want to use?\n",
    "\n",
    "We encourage the user to try different models by changing the parameter **name_model** to probe (for a given planet type, hot Jupiter or hot Neptune) the correlation with:\n",
    "- The exact same template as the one injected as a fiducial case\n",
    "- Models containing only 1 molecule (H2O, CO, CH4, NH3)\n",
    "\n",
    "For each model used for the correlation, we plot below the injected planet atmosphere 1D template (black) and the template used for the correlation (magenta). **What molecule can we detect from the data (H2O, NH3, CO, CH4)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate below the name of the planet atmosphere template injected in the data\n",
    "name_model    =  \"Model/Jupiter_1sol_Teq1500K_Tint150K\"\n",
    "#name_model    = \"Model/Neptune_30sol_Teq600K_Tint50K_CH4\"\n",
    "molec         = \"H2O\"\n",
    "\n",
    "aa            = np.loadtxt(name_model)\n",
    "W_mod         = np.array(1./aa[:,0]*1e7)[::-1]  ### Wavelength of the model [nm]\n",
    "transit_depth = np.array(aa[:,1])[::-1]         ### Transit depth\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(W_mod_inj,transit_depth_inj,\"-k\",lw=.5,label=\"injected\")\n",
    "plt.plot(W_mod,transit_depth,\"-m\",lw=.5,label=molec)\n",
    "plt.ylabel(\"Transit Depth\")\n",
    "plt.xlabel(\"Wavelength [nm]\")\n",
    "plt.legend(ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now select the orders we will use to compute the correlation on. In practice, we advocate to select all available orders exhibiting absorption features to avoid any selection bias in the process (except in the order's spectrum exhibit significant telluric contamination or correlated noise). In our case, we compute the absolute difference between the injected planet template and the model used for the correlation. If this difference is larger than 2%, we discard the order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the model to each relevant order\n",
    "discard           = np.zeros(nord_fin)\n",
    "list_ord_corr     = []\n",
    "orders_corr       = []\n",
    "index_orders_corr = []\n",
    "Wmin_corr         = []\n",
    "Wmax_corr         = []\n",
    "\n",
    "for kk,O in enumerate(list_ord_pl_fin):\n",
    "    #print(kk,O.W_red.min())\n",
    "    Wmin,Wmax = 0.95*O.W_red.min(),1.05*O.W_red.max()\n",
    "    indm      = np.where((W_mod>Wmin)&(W_mod<Wmax))[0]\n",
    "    W_sel     = W_mod[indm]\n",
    "    S         = np.std(transit_depth[indm])\n",
    "    diff_transit=np.abs(np.mean(transit_depth[indm])-np.mean(transit_depth_inj[indm]))/np.mean(transit_depth_inj[indm])    \n",
    "    #if np.min(W_sel) > 0.99*O.W_red.min() or np.max(W_sel) < 1.01*O.W_red.max() or S<1e-4:\n",
    "    #if np.min(W_sel) > 0.99*O.W_red.min() or np.max(W_sel) < 1.01*O.W_red.max() or diff_transit>0.02: \n",
    "    if diff_transit>0.02:         \n",
    "        print(\"Order\",O.number,\" - Wm = \",wmean[kk],\"nm - incomplete -- discarded (dispersion:\",round(S,8),\")\") \n",
    "    else:\n",
    "        ### If order selected, we save wavelength and 1-transit_depth in Order object's attributes\n",
    "        O.Wm = W_sel\n",
    "        O.Im = 1. - transit_depth[indm]\n",
    "        list_ord_corr.append(O)\n",
    "        orders_corr.append(O.number)\n",
    "        index_orders_corr.append(kk)\n",
    "        Wmin_corr.append(O.W_red.min())\n",
    "        Wmax_corr.append(O.W_red.max())        \n",
    "print(\"\\n\\nOrders ok for correlation with model\",molec,\":\")\n",
    "\n",
    "\n",
    "### Plot model and associated orders\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(W_mod_inj,transit_depth_inj,\"-k\",lw=.5,label=\"injected\")\n",
    "plt.plot(W_mod,transit_depth,\"-g\",lw=.5,label=molec)\n",
    "plt.ylabel(\"Transit Depth\")\n",
    "plt.xlabel(\"Wavelength [nm]\")\n",
    "plt.legend(ncol=2)\n",
    "amount=len(orders_corr)-1\n",
    "for kk,i in enumerate(orders_corr): \n",
    "    print(kk, Wmin_corr[kk], Wmax_corr[kk])\n",
    "    c = [float(kk)/float(amount), 0.0, float(amount-kk)/float(amount)]\n",
    "    plt.axvspan(Wmin_corr[kk], Wmax_corr[kk],color=c, alpha=0.5, lw=0) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########################################################################################\n",
    "\n",
    "**BONUS 3:** Select the orders used in the correlation process using a merit/weight order-dependent function? \n",
    "\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main parameters of the grid for the correlation process\n",
    "\n",
    "Select here the min/max values and the number of points for the (Kp,Vsys) grid. In this tutorial, we advocate to include no more than 20/30 steps for the Kp grid (for time purposes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters of the grid\n",
    "\n",
    "### Semi-amplitude of the planet RV [km/s]\n",
    "Kpmin = 50.0 #Jupiter\n",
    "Kpmax = 250.0#Jupiter\n",
    "#Kpmin = -50.0 #Neptune\n",
    "#Kpmax = 150.0 #Neptune\n",
    "\n",
    "Nkp   = 20 ### Size of the grid\n",
    "Kp    = np.linspace(Kpmin,Kpmax,Nkp)\n",
    "\n",
    "### Mid-transit planet RV [km/s]\n",
    "Vmin = -30.0\n",
    "Vmax =  30.0\n",
    "Nv   = 61\n",
    "Vsys = np.linspace(Vmin,Vmax,Nv)\n",
    "\n",
    "### Order selection for computing the correlation (either manually or automatically)\n",
    "#ord_sel = [50, 49, 48, 47, 46, 36, 35, 34]  #[70,58,47,46,32]  ### Manual seletion\n",
    "ord_sel=orders_corr ### Automatic selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute correlation maps\n",
    "\n",
    "**Takes some time**\n",
    "\n",
    "For each selected SPIRou order and for each sample of the (Kp,Vsys) grid:\n",
    "- Interpolate the planet atmosphere spectrum.\n",
    "- Build a sequence of planet atmosphere spectra by (1) shifting the planet atmosphere template according to the planet RV solution and weighting it by the transit window at each observation time, (2) bin each spectrum at the resolution of the SPIRou observations.\n",
    "- Compute the average correlation coefficient between the observed and synthetic spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_sel = []\n",
    "for kk,oo in enumerate(list_ord_corr):\n",
    "    if oo.number in ord_sel: ind_sel.append(kk)\n",
    "corr_pl = compute_correlation(np.array(list_ord_corr)[ind_sel],window,phase,Kp,Vsys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-analysis\n",
    "\n",
    "- Compute maximum significance\n",
    "- Compute error bars on Kp ad Vsys\n",
    "\n",
    "We first convert the correlation map into a significance map by dividing it by the typical dispersion of the noise in the correlation map. The latter is simply computed by taking the standard deviation of the correlation map, on regions where the planetary signal is expected to be (mostly) absent (note that this is a simplistic approach and that, in practice, additional statistical tests can ensure a more robust estimation of the noise level, see [Birkby 2018](https://ui.adsabs.harvard.edu/abs/2018arXiv180604617B/abstract).). **Note:** Another method to estimate the typical noise level in the correlation map would be to compute the correlation map between the planet atmosphere template and a 2D matrix containing only noise (possibly amplified at the extremity of each order to account for the blaze function). The typical noise level is then simply given by the dispersion of the obtained correlation map.\n",
    "\n",
    "We then fit a bivariate normal law to the significance map in the (Kp,Vsys) space and derive the best-fitting Kp and Vsys as well as the maximum significance. To compute the error bars on Kp and Vsys, we compute a densely-sampled bivariate normal law using the best-fitting parameters previously obtained. Then, starting from the maximum significance, we search the distance in the Kp space (resp. V_sys space) such that the significance decreases by 1 sigma (freezing the other parameter to its best estimate). Note that, if the grid is too small (and than no level lower than 1 sigma from the max is identified), we return the size of the grid as an error bar on the parameter as well as a warning message. In practice, we advocate to run the correlation maps on finely-sampled large grids in the (Kp,Vsys) space in order to ensure that the maximum of correlation is indeed global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Compute statistics and plot the map\n",
    "\n",
    "# Indicate regions to exclude when computing the NOISE level from the correlation map\n",
    "Kp_lim    = [110,190]   # Exclude this Kp range we\n",
    "Vsys_lim  = [-15.,15.]\n",
    "\n",
    "\n",
    "### Compute significance map\n",
    "snrmap_fin  = get_snrmap(np.array(orders_corr)[ind_sel],Kp,Vsys,corr_pl,Kp_lim,Vsys_lim)\n",
    "sig_pl_fin  = np.sum(corr_pl[:,:,:]/snrmap_fin,axis=2)\n",
    "\n",
    "### Get and display statistics\n",
    "p_best,K_best,K_sup,K_inf,V_best,V_sup,V_inf = get_statistics(Vsys,Kp,sig_pl_fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the correlation maps:\n",
    "- Color map showing the significance of the correlation with the planet template in the (Kp,Vsys) space\n",
    "- 1D cuts at the injected values of Kp and Vsys\n",
    "In each case, we indicate the injected values of Kp and Vsys by white dashed lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_cut   = K_inj\n",
    "V_cut   = V_inj\n",
    "ind_v   = np.argmin(np.abs(Vsys-V_cut))\n",
    "ind_k   = np.argmin(np.abs(Kp-K_cut))\n",
    "sn_map  = sig_pl_fin\n",
    "sn_cutx = sn_map[:,ind_v]\n",
    "sn_cuty = sn_map[ind_k]\n",
    "\n",
    "cmap = \"gist_heat\"\n",
    "plot_correlation_map(Vsys,Kp,sn_map,V_inj,K_inj,cmap)\n",
    "\n",
    "### Plot correlation + 1D cut\n",
    "plot_correlation_map(Vsys,Kp,sn_map,V_inj,K_inj,cmap,sn_cutx,sn_cuty,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########################################################################################\n",
    "\n",
    "**BONUS 4:** Using the planet atmosphere template *Jupiter_1sol_Teq1500K_Tint150K_superrot.txt*, try to probe the signature of super rotation in the planet atmosphere?\n",
    "\n",
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########################################################################################\n",
    "\n",
    "**BONUS 5:** By implementing a cross-correlation process between the planet atmosphere template and each observed spectrum, try to probe the mean planet signature in the (time,V_sys) space?\n",
    "\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
